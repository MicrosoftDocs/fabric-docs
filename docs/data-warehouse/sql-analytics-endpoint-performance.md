---
title: SQL Analytics Endpoint Performance Considerations
description: Learn more about performance considerations for the SQL analytics endpoint of a lakehouse in Microsoft Fabric.
author: WilliamDAssafMSFT
ms.author: wiassaf
ms.reviewer: procha, anphil, maprycem, amasingh
ms.date: 01/23/2026
ms.service: fabric
ms.subservice: data-warehouse
ms.topic: concept-article
ms.search.form: Optimization # This article's title should not change. If so, contact engineering.
---
# SQL analytics endpoint performance considerations

**Applies to:** [!INCLUDE [fabric-se](includes/applies-to-version/fabric-se.md)]

The [!INCLUDE [fabric-se](includes/fabric-se.md)] enables you to query data in the lakehouse using T-SQL language and TDS protocol. 

Every lakehouse has one [!INCLUDE [fabric-se](includes/fabric-se.md)]. The number of SQL analytics endpoints in a workspace matches the number of [lakehouses](../data-engineering/lakehouse-overview.md) and [mirrored databases](../mirroring/overview.md) provisioned in that one workspace.

A background process is responsible for scanning lakehouse for changes, and keeping [!INCLUDE [fabric-se](includes/fabric-se.md)] up-to-date for all the changes committed to lakehouses in a workspace. The sync process is transparently managed by Microsoft Fabric platform. When a change is detected in a lakehouse, a background process updates metadata and the [!INCLUDE [fabric-se](includes/fabric-se.md)] reflects the changes committed to lakehouse tables. Under normal operating conditions, the lag between a lakehouse and [!INCLUDE [fabric-se](includes/fabric-se.md)] is less than one minute. The actual length of time can vary from a few seconds to minutes depending on many factors that are discussed in this article. The background process runs only when the SQL analytics endpoint is active and it's halted after 15 minutes of inactivity.

### Automatically generated schema in the SQL analytics endpoint of the Lakehouse

The [!INCLUDE [fabric-se](includes/fabric-se.md)] manages the automatically generated tables so the workspace users can't modify them. Users can enrich the database model by adding their own SQL schemas, views, procedures, and other database objects.

For every Delta table in your [Lakehouse](../data-engineering/lakehouse-overview.md), the [!INCLUDE [fabric-se](includes/fabric-se.md)] automatically generates a table in the appropriate schema. For autogenerated schema data types for the [!INCLUDE [fabric-se](includes/fabric-se.md)], see [Data types in Microsoft Fabric](data-types.md#autogenerated-data-types-in-the-sql-analytics-endpoint).

Tables in the [!INCLUDE [fabric-se](includes/fabric-se.md)] are created with a minor delay. Once you create or update Delta Lake table in the lake, the [!INCLUDE [fabric-se](includes/fabric-se.md)] table that references the Delta lake table is created/refreshed automatically.

The amount of time it takes to refresh the table is related to how optimized the Delta tables are. For more information, review [Delta Lake table optimization and V-Order](../data-engineering/delta-optimization-and-v-order.md) to learn more about key scenarios, and an in-depth guide on how to efficiently maintain Delta tables for maximum performance.

You can manually force a refresh of the automatic metadata scanning in the Fabric portal. On the page for the [!INCLUDE [fabric-se](includes/fabric-se.md)], select the **Refresh** button in the **Explorer** toolbar to refresh the schema. Go to **Query** your SQL analytics endpoint, and look for the refresh button, as shown in the following image.

:::image type="content" source="media/sql-analytics-endpoint-performance/sql-endpoint-refresh-button.png" alt-text="Screenshot from the Fabric portal showing the SQL analytics endpoint Refresh schema button." lightbox="media/sql-analytics-endpoint-performance/sql-endpoint-refresh-button.png":::

You can also programmatically force a refresh of the automatic metadata scanning using the [Refresh SQL endpoint metadata REST API](/rest/api/fabric/sqlendpoint/items/refresh-sql-endpoint-metadata).

## Guidance

- Automatic metadata discovery tracks changes committed to lakehouses, and is a single instance per Fabric workspace. If you're observing increased latency for changes to sync between lakehouses and the [!INCLUDE [fabric-se](includes/fabric-se.md)], it could be due to large number of lakehouses in one workspace. In such a scenario, consider migrating each lakehouse to a separate workspace as this allows automatic metadata discovery to scale.
- Parquet files are immutable by design. When there's an update or a delete operation, a Delta table will add new parquet files with the changeset, increasing the number of files over time, depending on frequency of updates and deletes. If there's no maintenance scheduled, eventually, this pattern creates a read overhead and this impacts time it takes to sync changes to [!INCLUDE [fabric-se](includes/fabric-se.md)]. To address this, schedule regular [lakehouse table maintenance operations](../data-engineering/lakehouse-table-maintenance.md#execute-ad-hoc-table-maintenance-on-a-delta-table-using-lakehouse).
- In some scenarios, you might observe that changes committed to a lakehouse aren't visible in the associated [!INCLUDE [fabric-se](includes/fabric-se.md)]. For example, you might have created a new table in lakehouse, but it's not yet listed in the [!INCLUDE [fabric-se](includes/fabric-se.md)]. Or, you might have committed a large number of rows to a table in a lakehouse but this data isn't yet visible in the [!INCLUDE [fabric-se](includes/fabric-se.md)]. We recommend initiating an on-demand metadata sync, triggered from the SQL query editor **Refresh** ribbon option or the [Refresh SQL endpoint metadata REST API](/rest/api/fabric/sqlendpoint/items/refresh-sql-endpoint-metadata). This option forces an on-demand metadata sync, rather than waiting on the background metadata sync to finish.
- Not all Delta features are understood by the automatic sync process. For more information on the functionality supported by each engine in Fabric, see [Delta Lake table format interoperability](../fundamentals/delta-lake-interoperability.md).
- If there's an extremely large volume of tables changes during the Extract Transform and Load (ETL) processing, an expected delay could occur until all the changes are processed.

## Optimizing lakehouse tables for querying the SQL analytics endpoint

When the SQL analytics endpoint reads tables stored in a lakehouse, query performance is heavily influenced by the physical layout of the underlying parquet files. 

A large number of small parquet files creates overhead and negatively affects query performance. To ensure predictable and efficient performance, we recommend maintaining table storage so that each parquet file contains two million rows. This row count provides a balanced level of parallelism without fragmenting the dataset into excessively small slices. 

In addition to row count guidance, file size is equally important. The SQL analytics endpoint performs best when parquet files are large enough to minimize file handling overhead but not so large that they limit parallel scan efficiency. For most workloads, keeping individual parquet files close to 400 MB strikes the best balance. To achieve this balance, use the following steps: 

1. Set `maxRecordsPerFile` to 2,000,000 before data changes occurs.
1. Perform your data changes (data ingestion, updates, deletes).
1. Set `maxFileSize` to 400 MB.
1. Run `OPTIMIZE`. For details on using `OPTIMIZE`, refer to [Table maintenance operations](../data-engineering/lakehouse-table-maintenance.md#table-maintenance-operations).

The following script provides a template for these steps, and should be executed on a lakehouse: 

```python 
from delta.tables import DeltaTable

# 1. CONFIGURE LIMITS

# Cap files to 2M rows during writes. This should be done before data ingestion occurs. 
spark.conf.set("spark.sql.files.maxRecordsPerFile", 2000000)

# 2. INGEST DATA
# Here, you ingest data into your table 

# 3. CAP FILE SIZE (~400 MB)
spark.conf.set("spark.databricks.delta.optimize.maxFileSize", 400 * 1024 * 1024)

# 4. RUN OPTIMIZE (bin-packing)
spark.sql("""
    OPTIMIZE myTable
""")
```

To maintain healthy file sizes, users should periodically run Delta optimization operations such as OPTIMIZE, especially for tables that receive frequent incremental writes, updates and deletes. These maintenance operations compact small files into appropriately sized ones, helping ensure the SQL analytics endpoint can process queries efficiently.

> [!NOTE]
> For guidance on general maintenance of lakehouse tables, refer to [Execute ad-hoc table maintenance on a Delta table using Lakehouse](../data-engineering/lakehouse-table-maintenance.md#execute-ad-hoc-table-maintenance-on-a-delta-table-using-lakehouse).

## Partition size considerations

The choice of partition column for a delta table in a lakehouse also affects the time it takes to sync changes to [!INCLUDE [fabric-se](includes/fabric-se.md)]. The number and size of partitions of the partition column are important for performance:

- A column with high cardinality (mostly or entirely made of unique values) results in a large number of partitions. A large number of partitions negatively impacts performance of the metadata discovery scan for changes. If the cardinality of a column is high, choose another column for partitioning.
- The size of each partition can also affect performance. Our recommendation is to use a column that would result in a partition of at least (or close to) 1 GB. We recommend following best practices for [delta tables maintenance](../data-engineering/lakehouse-table-maintenance.md); [optimization](../data-engineering/delta-optimization-and-v-order.md). For a python script to evaluate partitions, see [Sample script for partition details](#sample-script-for-partition-details).

A large volume of small-sized parquet files increases the time it takes to sync changes between a lakehouse and its associated [!INCLUDE [fabric-se](includes/fabric-se.md)]. You might end up with large number of parquet files in a delta table for one or more reasons:

- If you choose a partition for a delta table with high number of unique values, it's partitioned by each unique value and might be over-partitioned. Choose a partition column that doesn't have a high cardinality, and results in individual partitions at least 1 GB each.
- Batch and streaming data ingestion rates might also result in small files depending on frequency and size of changes being written to a lakehouse. For example, there might be a small volume of changes coming through to the lakehouse, resulting in small parquet files. To address this, we recommend implementing regular [lakehouse table maintenance](../data-engineering/lakehouse-table-maintenance.md).
    
### Sample script for partition details

Use the following notebook to print a report detailing size and details of partitions underpinning a delta table.

1. First, you must provide the ABSFF path for your delta table in the variable `delta_table_path`.  
    - You can get ABFSS path of a delta table from the Fabric portal **Explorer**. Right-click on table name, then select `COPY PATH` from the list of options.
1. The script outputs all partitions for the delta table.
1. The script iterates through each partition to calculate the total size and number of files.
1. The script outputs the details of partitions, files per partitions, and size per partition in GB.

The complete script can be copied from the following code block:

  ```python
  # Purpose: Print out details of partitions, files per partitions, and size per partition in GB.
    from notebookutils import mssparkutils
  
  # Define ABFSS path for your delta table. You can get ABFSS path of a delta table by simply right-clicking on table name and selecting COPY PATH from the list of options.
    delta_table_path = "abfss://<workspace id>@<onelake>.dfs.fabric.microsoft.com/<lakehouse id>/Tables/<tablename>"
  
  # List all partitions for given delta table
  partitions = mssparkutils.fs.ls(delta_table_path)
  
  # Initialize a dictionary to store partition details
  partition_details = {}

  # Iterate through each partition
  for partition in partitions:
    if partition.isDir:
        partition_name = partition.name
        partition_path = partition.path
        files = mssparkutils.fs.ls(partition_path)
        
        # Calculate the total size of the partition

        total_size = sum(file.size for file in files if not file.isDir)
        
        # Count the number of files

        file_count = sum(1 for file in files if not file.isDir)
        
        # Write partition details

        partition_details[partition_name] = {
            "size_bytes": total_size,
            "file_count": file_count
        }
        
  # Print the partition details
  for partition_name, details in partition_details.items():
    print(f"{partition_name}, Size: {details['size_bytes']:.2f} bytes, Number of files: {details['file_count']}")

  ```

## Related content

- [Better together: the lakehouse and warehouse](get-started-lakehouse-sql-analytics-endpoint.md)
- [Fabric Data Warehouse performance guidelines](guidelines-warehouse-performance.md)
- [Limitations of the SQL analytics endpoint](limitations.md#limitations-of-the-sql-analytics-endpoint)
