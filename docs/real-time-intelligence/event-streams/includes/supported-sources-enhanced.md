---
title: Sources Supported by Fabric Eventstreams (Enhanced)
description: This file has the list of sources supported by Fabric eventstreams with enhanced capabilities.
ms.author: xujiang1
author: xujxu
ms.topic: include
ms.custom:
ms.date: 08/26/2025
---

| Source          | Description |
| --------------- | ---------- |
| [Azure Data Explorer (preview)](../add-source-azure-data-explorer-database.md) | If you have an Azure Data Explorer database and a table, you can ingest data from the table into Microsoft Fabric by using an eventstream.  |
| [Azure Event Hubs](../add-source-azure-event-hubs.md) | If you have an Azure event hub, you can ingest data from the event hub into Fabric by using an eventstream.  |
| [Azure Event Grid (preview)](../add-source-azure-event-grid.md) | If you have an Azure Event Grid namespace, you can ingest MQTT or non-MQTT event data into Fabric by using an eventstream. |
| [Azure Service Bus (preview)](../add-source-azure-service-bus.md) | You can ingest data from an Azure Service Bus queue or a topic's subscription into Fabric by using an eventstream.  |
| [Azure IoT Hub](../add-source-azure-iot-hub.md) | If you have an Azure IoT hub, you can ingest IoT data into Fabric by using an eventstream.  |
| [Custom endpoint (custom app in standard capability)](../add-source-custom-app.md) | The custom endpoint feature allows your applications or Kafka clients to connect to an eventstream by using a connection string. This ability enables the smooth ingestion of streaming data into the eventstream. |
| [Azure IoT Operations](/azure/iot-operations/connect-to-cloud/howto-configure-fabric-real-time-intelligence) | You can configure Azure IoT Operations to send real-time data directly to Fabric Real-Time Intelligence by using an eventstream custom endpoint. This ability supports Microsoft Entra ID or Simple Authentication and Security Layer (SASL) authentication.  |
| [Sample data](../add-source-sample-data.md) | You can choose **Bicycles**, **Yellow Taxi**, or **Stock Market** as a sample data source to test the data ingestion while you're setting up an eventstream. |
| [Real-time weather (preview)](../add-source-real-time-weather.md) | You can add a real-time weather source to an eventstream to stream real-time weather data from various locations. |
| [Azure SQL Database Change Data Capture (CDC)](../add-source-azure-sql-database-change-data-capture.md) | You can use the Azure SQL Database CDC source connector to capture a snapshot of the current data in an Azure SQL database. The connector then monitors and records any future row-level changes to this data. |
| [Azure Database for PostgreSQL CDC](../add-source-postgresql-database-change-data-capture.md) | You can use the Azure Database for PostgreSQL CDC source connector to capture a snapshot of the current data in an Azure Database for PostgreSQL database. The connector then monitors and records any future row-level changes to this data. |
| [Azure Database for MySQL CDC](../add-source-mysql-database-change-data-capture.md) | You can use the MySQL database CDC source connector to capture a snapshot of the current data in an Azure Database for MySQL database. You can specify the tables to monitor, and the eventstream records any future row-level changes to the tables. |
| [Azure Cosmos DB CDC](../add-source-azure-cosmos-db-change-data-capture.md) | You can use the Azure Cosmos DB CDC source connector to capture a snapshot of the current data in an Azure Cosmos DB database. The connector then monitors and records any future row-level changes to this data. |
| [SQL Server on virtual machine (VM) database (DB) CDC](../add-source-sql-server-change-data-capture.md) | You can use the SQL Server on VM DB CDC source connector to capture a snapshot of the current data in a SQL Server database on a VM. The connector then monitors and records any future row-level changes to the data. |
| [Azure SQL Managed Instance CDC](../add-source-azure-sql-managed-instance-change-data-capture.md) | You can use the Azure SQL Managed Instance CDC source connector to capture a snapshot of the current data in a SQL Managed Instance database. The connector then monitors and records any future row-level changes to this data.  |
| [Fabric workspace item events](../add-source-fabric-workspace.md) | Workspace item events are discrete Fabric events that occur when changes are made to your Fabric workspace. These changes include creating, updating, or deleting a Fabric item. With Fabric eventstreams, you can capture these Fabric workspace events, transform them, and route them to various destinations in Fabric for further analysis. |
| [Fabric OneLake events](../add-source-fabric-onelake.md) | You can use OneLake events to subscribe to changes in files and folders in OneLake, and then react to those changes in real time. With Fabric eventstreams, you can capture these OneLake events, transform them, and route them to various destinations in Fabric for further analysis. This seamless integration of OneLake events within Fabric eventstreams gives you greater flexibility for monitoring and analyzing activities in OneLake. |
| [Fabric job events](../add-source-fabric-job.md) | You can use job events to subscribe to changes produced when Fabric runs a job. For example, you can react to changes when you're refreshing a semantic model, running a scheduled pipeline, or running a notebook. Each of these activities can generate a corresponding job, which in turn generates a set of corresponding job events. With Fabric eventstreams, you can capture these job events, transform them, and route them to various destinations in Fabric for further analysis. This seamless integration of job events within Fabric eventstreams gives you greater flexibility for monitoring and analyzing activities in your job. |
| [Azure Blob Storage events](../add-source-azure-blob-storage.md) | Azure Blob Storage events are triggered when a client creates, replaces, or deletes a blob. You can use the connector to link Blob Storage events to Fabric events in a real-time hub. You can convert these events into continuous data streams and transform them before routing them to various destinations in Fabric.|
| [Google Cloud Pub/Sub](../add-source-google-cloud-pub-sub.md) | Google Cloud Pub/Sub is a messaging service for publishing and subscribing to streams of events. You can add Google Cloud Pub/Sub as a source to your eventstream to capture, transform, and route real-time events to various destinations in Fabric. |
| [Amazon Kinesis Data Streams](../add-source-amazon-kinesis-data-streams.md) | Amazon Kinesis Data Streams is a massively scalable, highly durable data ingestion and processing service that's optimized for streaming data. By integrating Amazon Kinesis Data Streams as a source within your eventstream, you can seamlessly process real-time data streams before routing them to multiple destinations within Fabric. |
| [Confluent Cloud for Apache Kafka](../add-source-confluent-kafka.md) | Confluent Cloud for Apache Kafka is a streaming platform that offers powerful data streaming and processing functionalities that use Apache Kafka. By integrating Confluent Cloud for Apache Kafka as a source within your eventstream, you can seamlessly process real-time data streams before routing them to multiple destinations within Fabric. |
| [Apache Kafka (preview)](../add-source-apache-kafka.md) | Apache Kafka is an open-source, distributed platform for building scalable, real-time data systems. By integrating Apache Kafka as a source within your eventstream, you can seamlessly bring real-time events from Apache Kafka and process them before routing them to multiple destinations within Fabric. |
| [Amazon MSK Kafka](../add-source-amazon-managed-streaming-for-apache-kafka.md) | Amazon MSK Kafka is a fully managed Kafka service that simplifies setup, scaling, and management. By integrating Amazon MSK Kafka as a source within your eventstream, you can seamlessly bring real-time events from MSK Kafka and process them before routing them to multiple destinations within Fabric. |
| [MQTT (preview)](../add-source-mqtt.md) | You can use Fabric eventstreams to connect to an MQTT broker. Messages in the MQTT broker can then be ingested into a Fabric eventstream and routed to various destinations within Fabric. |
| [Solace PubSub+ (preview)](../add-source-solace-pub-sub.md) | You can use Fabric eventstreams to connect to Solace PubSub+. Messages from Solace PubSub+ can then be ingested into a Fabric eventstream and routed to various destinations within Fabric. |
